{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stAIner1988/NLP/blob/main/Projekt_XX_German_News_Article1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dZiMvk-NxZd"
      },
      "source": [
        "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
        "\n",
        "\n",
        "# German News Articles\n",
        "\n",
        "by Joel Akeret and Fabian Märki\n",
        "\n",
        "## Summary\n",
        "This is a short intro on how to access the *Ten Thousand German News Articles Dataset* for the *default project* (we still encourage you to work with your own dataset).\n",
        "\n",
        "## Links\n",
        "- [Ten Thousand German News Articles Dataset](https://tblock.github.io/10kGNAD/)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/markif/2023_HS_DAS_NLP_Notebooks/blob/master/XX_German_News_Article.ipynb\">\n",
        "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('https://drive.google.com/drive/folders/1kOacBkp9_6oESaMyhYlnO2PL-rke2p48?usp=share_link')"
      ],
      "metadata": {
        "id": "F8L8Ig3QErhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuer Abschnitt"
      ],
      "metadata": {
        "id": "dTXu31QAC-E0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsSGzb_JNxZf"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "\n",
        "!pip install 'fhnw-nlp-utils>=0.8.0,<0.9.0'\n",
        "!pip install transformers\n",
        "from fhnw.nlp.utils.processing import parallelize_dataframe\n",
        "from fhnw.nlp.utils.processing import is_iterable\n",
        "from fhnw.nlp.utils.storage import download\n",
        "from fhnw.nlp.utils.storage import save_dataframe\n",
        "from fhnw.nlp.utils.storage import load_dataframe\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "tqdm.tqdm.pandas()\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuer Abschnitt"
      ],
      "metadata": {
        "id": "-_ErmEKDypCy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndo9LK50NxZh"
      },
      "source": [
        "We recommend to use the stratified train/test split proposed by the maintainer of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my-fhcECNxZh"
      },
      "outputs": [],
      "source": [
        "file = \"data/german_news_articles_original_train.parq\"\n",
        "download(\"https://drive.switch.ch/index.php/s/mRnuzx4BLpMLqyz/download\", file)\n",
        "data_train = load_dataframe(file)\n",
        "\n",
        "file = \"data/german_news_articles_original_test.parq\"\n",
        "download(\"https://drive.switch.ch/index.php/s/DKUnZraeGp3EIK3/download\", file)\n",
        "data_test = load_dataframe(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joZqun3ANxZh"
      },
      "source": [
        "Get a first impression..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK-sgEwzNxZh"
      },
      "outputs": [],
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8OjknCxNxZi"
      },
      "outputs": [],
      "source": [
        "data_train[\"split\"] = \"train\"\n",
        "data_test[\"split\"] = \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJraAxsENxZi"
      },
      "outputs": [],
      "source": [
        "data_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW0ntFbVNxZi"
      },
      "outputs": [],
      "source": [
        "data_test.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmvx5seBNxZi"
      },
      "outputs": [],
      "source": [
        "data_all = pd.concat([data_train, data_test])\n",
        "data = data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_0D8F8LNxZj"
      },
      "outputs": [],
      "source": [
        "data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhQafcRiNxZj"
      },
      "outputs": [],
      "source": [
        "data_all['text_original'].str.len().plot(kind = 'hist', bins = 50)\n",
        "print(len(data_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lu5z5NXzLMK"
      },
      "outputs": [],
      "source": [
        "data_all['label'].value_counts().plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLUP_4h5Ka_1"
      },
      "source": [
        "https://www.kaggle.com/datasets/tblock/10kgnad/code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NABcLavoKZ1o"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KbCG27rKd_-"
      },
      "outputs": [],
      "source": [
        "for col in data_all.columns:\n",
        "    print(f\"Missing data for column [{col}]: {data_all[col].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75vLD2u8KwmJ"
      },
      "outputs": [],
      "source": [
        "df_all = data_all[\"label\"].value_counts().sort_index()\n",
        "df_train = data_train[\"label\"].value_counts().sort_index()\n",
        "df_test = data_test[\"label\"].value_counts().sort_index()\n",
        "df = pd.concat([df_all.to_frame(), df_train.to_frame(), df_test.to_frame()], axis=1)#, keys=[\"all\",\"train\",\"test\"])\n",
        "df.columns=[\"all\",\"train\",\"test\"]\n",
        "print (df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ardx_8fKwjM"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, keep_punctuation=False):\n",
        "    \"\"\"Cleans text by removing html tags, non ascii chars, digits and optionally punctuation\"\"\"\n",
        "\n",
        "    import re\n",
        "\n",
        "    # Compile RE pattern for HTTPS address, then Substitute it for blank\n",
        "    RE_HTTPS = re.compile(r\"https?://\\S+ \")\n",
        "    text = re.sub(RE_HTTPS, \"\", text)\n",
        "\n",
        "    # Subsitute twitter picutures for blank\n",
        "    text = re.sub(r'pic.twitter.com/[\\w]*',\"\", text)\n",
        "\n",
        "    # Subsitute multiple points space for 1 point\n",
        "    text = re.sub(r\"\\(?[.][.]+\\)?\", \".\", text)\n",
        "\n",
        "    # Compile RE pattern for HTML tags, then Substitute it for blank\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "\n",
        "    # Compile RE patterns for general text, including punctuation rule\n",
        "    if keep_punctuation:\n",
        "        RE_ASCII = re.compile(r\"[^a-züöä,.!?]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[a-züöä,.!?]\\b\", re.IGNORECASE)\n",
        "    else:\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "\n",
        "    # keep only ASCII + European Chars and whitespace, no digits\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    # convert all whitespaces (tabs etc.) to single wspace\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "\n",
        "    # Subsitute multiple blank space for 1 blank space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # Subsitute double punctuation (left-over after previous subsitutions) for 1 point\n",
        "    text = re.sub(r\" [.,]+ [,.]+\", \".\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVCmZEmPNDCt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH2GdUoRKwgT"
      },
      "outputs": [],
      "source": [
        "data_all = data_all.drop([\"text\"], axis=1, errors='ignore')\n",
        "\n",
        "# Apply text cleaning using MODIN pandas dataframe (parallelized)\n",
        "data_all[\"text\"] =  data_all[\"text_original\"].apply(clean_text, keep_punctuation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQfzgmNPKwdC"
      },
      "outputs": [],
      "source": [
        "data_all.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-o9tIdUOINP"
      },
      "outputs": [],
      "source": [
        "data_all.loc[:,['text','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0uWKQ8E1fHu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKXlbiW1Nt6t"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAXLEN = 192\n",
        "\n",
        "def tokenize(data_all):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text= data_all,\n",
        "        add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
        "        max_length=MAXLEN,  # Max length to truncate/pad\n",
        "        padding='max_length',  # Pad sentence to max length\n",
        "        return_attention_mask=False,  # attention mask not needed for our task\n",
        "        return_token_type_ids=False,\n",
        "        truncation=True, )\n",
        "\n",
        "    return encoded['input_ids']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL7uoVbXU8WM"
      },
      "outputs": [],
      "source": [
        "data_train = data_all[data_all['split'] == 'train'].loc[:,['text','label']]\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OpaRTwDVqRd"
      },
      "outputs": [],
      "source": [
        "data_test = data_all[data_all['split'] == 'test'].loc[:,['text','label']]\n",
        "data_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhhc4LT102oV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keB3E-UF0png"
      },
      "outputs": [],
      "source": [
        "input_ids_train = np.array([tokenize(data_train) for data_train in tqdm.tqdm(data_train['text'])])\n",
        "input_ids_test = np.array([tokenize(data_test) for data_test in tqdm.tqdm(data_test['text'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSmOClen3UQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "label_binarizer = LabelBinarizer()\n",
        "label_binarizer.fit(data_all[\"label\"])\n",
        "print(f\"Classes: {label_binarizer.classes_}\")\n",
        "print (f\"Encoding:\\n {label_binarizer.transform(label_binarizer.classes_).T}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYDLwWro3Cyq"
      },
      "outputs": [],
      "source": [
        "train_ids, test_ids, train_labels, test_labels = input_ids_train, input_ids_test, label_binarizer.transform(data_train['label']),label_binarizer.transform(data_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNS1JKq33B40"
      },
      "outputs": [],
      "source": [
        "#train_labels.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qde2jXIV2xQn"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1wDuEsX4d5x"
      },
      "outputs": [],
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices((train_ids, train_labels))\n",
        "                    .shuffle(buffer_size=len(train_ids), reshuffle_each_iteration=True)\n",
        "                    .repeat(EPOCHS)\n",
        "                    .batch(BATCH_SIZE))\n",
        "\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((test_ids, test_labels))\n",
        "                    .batch(BATCH_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w7SZYzY4pYH"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = len(pd.DataFrame(train_labels).drop_duplicates())\n",
        "NUM_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5mZ5ug48FY"
      },
      "outputs": [],
      "source": [
        "def build_model(max_len=MAXLEN): #                dropout_rate=0.2):\n",
        "    \"\"\" add multiclass classification to pretrained model\n",
        "    \"\"\"\n",
        "\n",
        "    input_word_ids = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
        "    )\n",
        "\n",
        "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
        "    encoder_outputs = bert_model(input_word_ids)\n",
        "\n",
        "    ##########################\n",
        "    ## YOUR CODE HERE START ##\n",
        "    ##########################\n",
        "\n",
        "    # Either use last_hidden_state use pooler_output\n",
        "    # that were returned in encoder_outputs\n",
        "    last_hidden_state = encoder_outputs[0]\n",
        "    pooler_output = encoder_outputs[1]\n",
        "\n",
        "    # In this case we will use the cls_embedding\n",
        "    cls_embedding = pooler_output\n",
        "    # Adding a Dropout layer\n",
        "    #dropout_layer = tf.keras.layers.Dropout(rate=dropout_rate)(cls_embedding)\n",
        "\n",
        "    # Create a feed-forward neural network with one hidden layer\n",
        "    hidden = tf.keras.layers.Dense(128, activation='relu')(cls_embedding)\n",
        "    # Change the output dimension to match the number of classes\n",
        "    output = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(hidden)\n",
        "\n",
        "    ##########################\n",
        "    ## YOUR CODE HERE END ##\n",
        "    ##########################\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V3Xsj5l4pHI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZpiH1gg-vzW"
      },
      "outputs": [],
      "source": [
        "model = build_model(max_len=MAXLEN)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EMOmzPc5ERe"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss = loss=\"categorical_crossentropy\"\n",
        "\n",
        "model.compile(optimizer, loss=loss, metrics=[\"accuracy\"], jit_compile=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJBtkfYXyOxT",
        "outputId": "0e21cf60-b40c-4489-beab-e6adc2409980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "577/577 [==============================] - 423s 632ms/step - loss: 0.6240 - accuracy: 0.7979 - val_loss: 0.3625 - val_accuracy: 0.8852\n",
            "Epoch 2/8\n",
            " 91/577 [===>..........................] - ETA: 4:49 - loss: 0.2602 - accuracy: 0.9190"
          ]
        }
      ],
      "source": [
        "from datetime import datetime  # Stellen Sie sicher, dass Sie datetime importiert haben\n",
        "\n",
        "# Ändern Sie den Log-Pfad\n",
        "log_dir = 'logs/' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset,\n",
        "    steps_per_epoch=int(np.floor((len(input_ids_train) / BATCH_SIZE))),\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", verbose=1, patience=1, restore_best_weights=True),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=log_dir)  # Verwenden Sie den geänderten Log-Pfad\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0TmWOwz8NoH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!pip install session-info\n",
        "import session_info\n",
        "session_info.show()\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQC2Ygxl6-4y"
      },
      "outputs": [],
      "source": [
        "history = pd.DataFrame({'epoch': hist.epoch, **hist.history}).set_index('epoch')\n",
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CvjgC1Abhlt"
      },
      "source": [
        "\t0.014772\t0.995451\t0.537464\t0.899805 war vorher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj-tK5rxAnM6"
      },
      "outputs": [],
      "source": [
        "history.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0zQKdmXNAuX"
      },
      "outputs": [],
      "source": [
        "result = model.predict(test_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPE4rbVKTMwW"
      },
      "outputs": [],
      "source": [
        "result = result > .5\n",
        "y_pred = result.astype(int)\n",
        "y_true = test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfGjbmB_WYbU"
      },
      "outputs": [],
      "source": [
        "l = list(label_binarizer.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFpELcudWgsV"
      },
      "outputs": [],
      "source": [
        "conf = multilabel_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "conf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "tf_model_from_google = { \"model\": model, \"result_test\": result, 'label_binarizer' : label_binarizer }\n",
        "pickle.dump( tf_model_from_google, open( r\"C:\\Users\\reto.steiner\\Desktop\\NLP\\nlp.p\", \"wb\" ))"
      ],
      "metadata": {
        "id": "9lXvqsla6MKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_2_GTjTCNQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZIFafmwCNgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}