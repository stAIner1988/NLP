{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dZiMvk-NxZd"
      },
      "source": [
        "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
        "\n",
        "\n",
        "# German News Articles - Data inspection and baseline\n",
        "\n",
        "Reto Steiner\n",
        "\n",
        "\n",
        "## Links\n",
        "- [Ten Thousand German News Articles Dataset](https://tblock.github.io/10kGNAD/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsSGzb_JNxZf",
        "outputId": "ee39aeb5-adc8-41a8-c425-c2b9c00857db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Das System kann die angegebene Datei nicht finden.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reto.steiner\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "\n",
        "!pip install 'fhnw-nlp-utils>=0.8.0,<0.9.0'\n",
        "!pip install transformers\n",
        "from fhnw.nlp.utils.processing import parallelize_dataframe\n",
        "from fhnw.nlp.utils.processing import is_iterable\n",
        "from fhnw.nlp.utils.storage import download\n",
        "from fhnw.nlp.utils.storage import save_dataframe\n",
        "from fhnw.nlp.utils.storage import load_dataframe\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import re\n",
        "tqdm.tqdm.pandas()\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "my-fhcECNxZh"
      },
      "outputs": [],
      "source": [
        "file = \"data/german_news_articles_original_train.parq\"\n",
        "download(\"https://drive.switch.ch/index.php/s/mRnuzx4BLpMLqyz/download\", file)\n",
        "data_train = load_dataframe(file)\n",
        "\n",
        "file = \"data/german_news_articles_original_test.parq\"\n",
        "download(\"https://drive.switch.ch/index.php/s/DKUnZraeGp3EIK3/download\", file)\n",
        "data_test = load_dataframe(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kK-sgEwzNxZh"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, keep_punctuation=False):\n",
        "    \"\"\"Cleans text by removing html tags, non ascii chars, digits and optionally punctuation\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Compile RE pattern for HTTPS address, then Substitute it for blank\n",
        "    RE_HTTPS = re.compile(r\"https?://\\S+ \")\n",
        "    text = re.sub(RE_HTTPS, \"\", text)\n",
        "\n",
        "    # Subsitute twitter picutures for blank\n",
        "    text = re.sub(r'pic.twitter.com/[\\w]*',\"\", text)\n",
        "\n",
        "    # Subsitute multiple points space for 1 point\n",
        "    text = re.sub(r\"\\(?[.][.]+\\)?\", \".\", text)\n",
        "\n",
        "    # Compile RE pattern for HTML tags, then Substitute it for blank\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "\n",
        "    # Compile RE patterns for general text, including punctuation rule\n",
        "    if keep_punctuation:\n",
        "        RE_ASCII = re.compile(r\"[^a-züöä,.!?]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[a-züöä,.!?]\\b\", re.IGNORECASE)\n",
        "    else:\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "\n",
        "    # keep only ASCII + European Chars and whitespace, no digits\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    # convert all whitespaces (tabs etc.) to single wspace\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "\n",
        "    # Subsitute multiple blank space for 1 blank space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # Subsitute double punctuation (left-over after previous subsitutions) for 1 point\n",
        "    text = re.sub(r\" [.,]+ [,.]+\", \".\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y8OjknCxNxZi"
      },
      "outputs": [],
      "source": [
        "data_train[\"split\"] = \"train\"\n",
        "data_test[\"split\"] = \"test\"\n",
        "data_all = pd.concat([data_train, data_test])\n",
        "data = data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LH2GdUoRKwgT"
      },
      "outputs": [],
      "source": [
        "data_all = data_all.drop([\"text\"], axis=1, errors='ignore')\n",
        "\n",
        "# Apply text cleaning using MODIN pandas dataframe (parallelized)\n",
        "data_all[\"text\"] =  data_all[\"text_original\"].apply(clean_text, keep_punctuation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mKXlbiW1Nt6t"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
        "MAXLEN = 192\n",
        "\n",
        "def tokenize(data_all):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text= data_all,\n",
        "        add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
        "        max_length=MAXLEN,  # Max length to truncate/pad\n",
        "        padding='max_length',  # Pad sentence to max length\n",
        "        return_attention_mask=False,  # attention mask not needed for our task\n",
        "        return_token_type_ids=False,\n",
        "        truncation=True, )\n",
        "\n",
        "    return encoded['input_ids']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WL7uoVbXU8WM"
      },
      "outputs": [],
      "source": [
        "data_train = data_all[data_all['split'] == 'train'].loc[:,['text','label']]\n",
        "data_test = data_all[data_all['split'] == 'test'].loc[:,['text','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keB3E-UF0png",
        "outputId": "2d2b28b8-c0e1-4f31-b439-91c8f5209ece"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9245 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9245/9245 [00:53<00:00, 172.84it/s]\n",
            "100%|██████████| 1028/1028 [00:06<00:00, 169.55it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_train = np.array([tokenize(data_train) for data_train in tqdm.tqdm(data_train['text'])])\n",
        "input_ids_test = np.array([tokenize(data_test) for data_test in tqdm.tqdm(data_test['text'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSmOClen3UQI",
        "outputId": "459b1ca8-8da7-497c-b4a2-d7719ad08646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['Etat' 'Inland' 'International' 'Kultur' 'Panorama' 'Sport' 'Web'\n",
            " 'Wirtschaft' 'Wissenschaft']\n",
            "Encoding:\n",
            " [[1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "label_binarizer = LabelBinarizer()\n",
        "label_binarizer.fit(data_all[\"label\"])\n",
        "print(f\"Classes: {label_binarizer.classes_}\")\n",
        "print (f\"Encoding:\\n {label_binarizer.transform(label_binarizer.classes_).T}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XYDLwWro3Cyq"
      },
      "outputs": [],
      "source": [
        "train_ids, test_ids, train_labels, test_labels = input_ids_train, input_ids_test, label_binarizer.transform(data_train['label']),label_binarizer.transform(data_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qde2jXIV2xQn"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "LEARNING_RATE = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R1wDuEsX4d5x"
      },
      "outputs": [],
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices((train_ids, train_labels))\n",
        "                    .shuffle(buffer_size=len(train_ids), reshuffle_each_iteration=True)\n",
        "                    .repeat(EPOCHS)\n",
        "                    .batch(BATCH_SIZE))\n",
        "\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices((test_ids, test_labels))\n",
        "                    .batch(BATCH_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w7SZYzY4pYH",
        "outputId": "e3ba9bfc-0b26-4038-e078-a02535a0ff9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NUM_CLASSES = len(pd.DataFrame(train_labels).drop_duplicates())\n",
        "NUM_CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5oWTB-Ay_Hx"
      },
      "source": [
        "Loss steigt -> hohe learning rate, +/- vertauscht\n",
        "\n",
        "Loss explodiert -> hohe lr, numerisches problem\n",
        "\n",
        "Loss osziliert -> hohe lr, data / label fehlerhaft geladen\n",
        "\n",
        "Loss plateau -> tiefe lr, backprob error, falscher Input in Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m5mZ5ug48FY",
        "outputId": "c0aed6d2-5eaa-4da9-be79-7adc52f1febe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "def build_regularized_model(max_len=MAXLEN):\n",
        "    input_word_ids = tf.keras.layers.Input(\n",
        "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
        "    )\n",
        "\n",
        "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
        "    encoder_outputs = bert_model(input_word_ids)\n",
        "\n",
        "    last_hidden_state = encoder_outputs[0]\n",
        "    pooler_output = encoder_outputs[1]\n",
        "\n",
        "    cls_embedding = pooler_output\n",
        "\n",
        "    # Add more hidden layers with L2-Regularization\n",
        "    hidden1 = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_embedding)\n",
        "    hidden2 = tf.keras.layers.Dense(128, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01))(hidden1)\n",
        "\n",
        "    # Add Dropout layers\n",
        "    dropout1 = tf.keras.layers.Dropout(0.5)(hidden1)\n",
        "    dropout2 = tf.keras.layers.Dropout(0.5)(hidden2)\n",
        "\n",
        "    # Change the output dimension to match the number of classes\n",
        "    output = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(dropout2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
        "\n",
        "    # Compile the model with a reduced learning rate for overfitting\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=LEARNING_RATE,\n",
        "        decay_steps=10000,\n",
        "        decay_rate=0.9\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    loss = \"categorical_crossentropy\"\n",
        "\n",
        "    model.compile(optimizer, loss=loss, metrics=[\"accuracy\"], jit_compile=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the regularized model\n",
        "regularized_model = build_regularized_model(max_len=MAXLEN)\n",
        "\n",
        "# Now you can train the regularized model using your training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EMOmzPc5ERe",
        "outputId": "3f818a10-02e2-4e1f-c381-63ef0890fc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  1/144 [..............................] - ETA: 6:05:24 - loss: 7.8785 - accuracy: 0.2031"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "log_dir = 'logs/' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "# Train the model for a large number of epochs\n",
        "hist = regularized_model.fit(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset,\n",
        "    steps_per_epoch=int(np.floor((len(input_ids_train) / BATCH_SIZE))),\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", verbose=1, patience=1, restore_best_weights=True),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "    ],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQC2Ygxl6-4y"
      },
      "outputs": [],
      "source": [
        "history = pd.DataFrame({'epoch': hist.epoch, **hist.history}).set_index('epoch')\n",
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj-tK5rxAnM6"
      },
      "outputs": [],
      "source": [
        "history.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0zQKdmXNAuX"
      },
      "outputs": [],
      "source": [
        "result = model.predict(test_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPE4rbVKTMwW"
      },
      "outputs": [],
      "source": [
        "result = result > .5\n",
        "y_pred = result.astype(int)\n",
        "y_true = test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfGjbmB_WYbU"
      },
      "outputs": [],
      "source": [
        "l = list(label_binarizer.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFpELcudWgsV"
      },
      "outputs": [],
      "source": [
        "\n",
        "conf = multilabel_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCYtyxvj00yg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\"\"\"\n",
        "f1_score(y_true, y_pred, average='macro')\n",
        "f1_score(y_true, y_pred, average='micro')\n",
        "f1_score(y_true, y_pred, average='weighted')\n",
        "\"\"\"\n",
        "f1_score(y_true, y_pred, average=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02iUNYmCvBBH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "x= {'model' : model, 'result_test' : result, 'LabelBinarizer' : LabelBinarizer};\n",
        "output = open('/content/drive/MyDrive/Colab Notebooks/model.pkl', 'wb')\n",
        "pickle.dump(x,output)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
