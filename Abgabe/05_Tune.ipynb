{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "#!pip install 'fhnw-nlp-utils>=0.8.0,<0.9.0'\n",
    "#!pip install transformers\n",
    "from fhnw.nlp.utils.processing import parallelize_dataframe\n",
    "from fhnw.nlp.utils.processing import is_iterable\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.storage import save_dataframe\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "#from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import re\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from fhnw.nlp.utils.preprocess import preprocess\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score ,precision_score, make_scorer, recall_score\n",
    "tqdm.tqdm.pandas()\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "pd.options.display.max_colwidth = 600\n",
    "pd.options.display.max_rows = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/german_news_articles_original_train.parq\"\n",
    "download(\"https://drive.switch.ch/index.php/s/mRnuzx4BLpMLqyz/download\", file)\n",
    "data_train = load_dataframe(file)\n",
    "\n",
    "file = \"data/german_news_articles_original_test.parq\"\n",
    "download(\"https://drive.switch.ch/index.php/s/DKUnZraeGp3EIK3/download\", file)\n",
    "data_test = load_dataframe(file)\n",
    "\n",
    "\n",
    "data_train[\"split\"] = \"train\"\n",
    "data_test[\"split\"] = \"test\"\n",
    "data_all = pd.concat([data_train, data_test])\n",
    "data = data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.drop([\"text\"], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label_code = le.fit(data_train['label'].drop_duplicates())\n",
    "#y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kultur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wissenschaft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label\n",
       "0          Sport\n",
       "1         Kultur\n",
       "2            Web\n",
       "3     Wirtschaft\n",
       "4         Inland\n",
       "5           Etat\n",
       "6  International\n",
       "7       Panorama\n",
       "8   Wissenschaft"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.DataFrame(data_train['label'].drop_duplicates(inplace=False))\n",
    "label.reset_index(inplace = True, drop = True)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorbereiten der Daten und kurzer check der längen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, keep_punctuation=False):\n",
    "    \"\"\"Cleans text by removing html tags, non ascii chars, digits and optionally punctuation\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Compile RE pattern for HTTPS address, then Substitute it for blank\n",
    "    RE_HTTPS = re.compile(r\"https?://\\S+ \")\n",
    "    text = re.sub(RE_HTTPS, \"\", text)\n",
    "\n",
    "    # Subsitute twitter picutures for blank\n",
    "    text = re.sub(r'pic.twitter.com/[\\w]*',\"\", text)\n",
    "\n",
    "    # Subsitute multiple points space for 1 point\n",
    "    text = re.sub(r\"\\(?[.][.]+\\)?\", \".\", text)\n",
    "\n",
    "    # Compile RE pattern for HTML tags, then Substitute it for blank\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "\n",
    "    # Compile RE patterns for general text, including punctuation rule\n",
    "    if keep_punctuation:\n",
    "        RE_ASCII = re.compile(r\"[^a-züöä,.!?]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[a-züöä,.!?]\\b\", re.IGNORECASE)\n",
    "    else:\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "\n",
    "    # keep only ASCII + European Chars and whitespace, no digits\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    # convert all whitespaces (tabs etc.) to single wspace\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "\n",
    "    # Subsitute multiple blank space for 1 blank space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Subsitute double punctuation (left-over after previous subsitutions) for 1 point\n",
    "    text = re.sub(r\" [.,]+ [,.]+\", \".\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9245 1028 1028\n"
     ]
    }
   ],
   "source": [
    "X_train = data['text_original'] #.apply(clean_text, keep_punctuation = True) -> getestet, gibt schlechtere Resultate\n",
    "X_test = data_test['text_original'] #.apply(clean_text, keep_punctuation = True)-> getestet, gibt schlechtere Resultate\n",
    "y_train = label_code.transform(data['label'])\n",
    "y_true = label_code.transform(data_test['label'])\n",
    "print(len(y_train),len(y_true), len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Regulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NEURON = 2**5\n",
    "l2_reg = 1e-3\n",
    "DROP = .5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntfidf_vectorizer = tf.keras.layers.TextVectorization(\\n    max_tokens=vocab_size,\\n    output_mode='tf-idf',\\n    ngrams=(2, 3)\\n)\\ntfidf_vectorizer.adapt(X_train.values)\\n# Define the neural network architecture\\nmodel = tf.keras.Sequential([\\n    tfidf_vectorizer,\\n    tf.keras.layers.Dense(32, activation='relu', input_shape=(vocab_size,)),  # Hidden layer with ReLU activation\\n    tf.keras.layers.Dense(9, activation='softmax')  # Output layer with softmax activation for multi-class classification\\n])\\n\\n# Compile the model\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.summary()\\n\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=None,\n",
    "    idf_weights=None,\n",
    "    sparse=False,\n",
    "    ragged=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "count_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='count'\n",
    ")\n",
    "\n",
    "count_vectorizer.adapt(X_train.values)\n",
    "\n",
    "\"\"\"\n",
    "tfidf_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='tf-idf',\n",
    "    ngrams=(2, 3)\n",
    ")\n",
    "tfidf_vectorizer.adapt(X_train.values)\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tfidf_vectorizer,\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(vocab_size,)),  # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(9, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = (\\ntest_dataset = (tf.data.Dataset.from_tensor_slices((X_test.values, y_true))\\n                    .batch(BATCH_SIZE))\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_dataset = (\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((X_test.values, y_true))\n",
    "                    .batch(BATCH_SIZE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(data_all[\"label\"])\n",
    "y_true=label_binarizer.transform(data_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "# Build the model\n",
    "def create_model(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "        count_vectorizer,\n",
    "        \n",
    "        tf.keras.layers.Dense(2*NEURON, activation='relu', input_shape=(vocab_size,), kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(hp.Choice('units', [.5, .2])),  # Adding dropout for regularization\n",
    "        tf.keras.layers.Dense(NEURON, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(hp.Choice('units', [.5, .2])),\n",
    "        tf.keras.layers.Dense(NEURON, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(hp.Choice('units', [.5, .2])),\n",
    "        tf.keras.layers.Dense(NEURON//2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(hp.Choice('units', [.5, .2])),\n",
    "        tf.keras.layers.Dense(9, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Compile the model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvectorizer = CountVectorizer()\\nX = vectorizer.fit_transform(X_train).toarray()\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_train).toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from logs\\intro_to_kt\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(create_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='logs',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_dataset, epochs=EPOCHS, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (Text  (None, 10000)             0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                640064    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 9)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643881 (2.46 MB)\n",
      "Trainable params: 643881 (2.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 6ms/step\n",
      "Accuracy: 0.0\n",
      "F1-Score: 0.0\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reto.steiner\\.pyenv\\pyenv-win\\versions\\3.9.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_test.values)\n",
    "result = result > .5\n",
    "y_pred = result.astype(int)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred,average='macro')\n",
    "#roc_auc = roc_auc_score(y_true, y_pred, average='ovs')\n",
    "prec = precision_score(y_true, y_pred,average='macro')\n",
    "rec = recall_score(y_true, y_pred,average='macro')\n",
    "print('Accuracy:', acc)\n",
    "print('F1-Score:', f1)\n",
    "#print('ROC-AUC:', roc_auc)\n",
    "print('Recall:', rec)\n",
    "print('Precision:', prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuner funktioniert nicht richtig und keine Zeit mehr. Auch um Model zu squeezen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
